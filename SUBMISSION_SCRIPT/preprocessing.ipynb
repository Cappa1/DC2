{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train, test):\n",
    "    \"\"\" \n",
    "    Function that initalizes the data\n",
    "    param: paths to train and test of the csv files\n",
    "    return: combined dataframe with clean column names\n",
    "    \"\"\"\n",
    "    \n",
    "    df_train = pd.read_csv(train)\n",
    "    df_test = pd.read_csv(test)\n",
    "    data = pd.concat([df_train, df_test])\n",
    "    data.columns = data.columns.str.strip()\n",
    "    if not DEMO:\n",
    "        pass\n",
    "    else:\n",
    "        data = data_split(data)\n",
    "    return data\n",
    "\n",
    "def data_split(df):\n",
    "    \"\"\"returns 10% of the data\"\"\"\n",
    "    return df[: int((len(df)/10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, labels=[\"event time:timestamp\", \"time of day\", \"weekday\"], time=True):\n",
    "    \"\"\"\n",
    "    Preprocesses all the data\n",
    "    param: a dataframe\n",
    "    return \n",
    "    \"\"\"\n",
    "    if time:\n",
    "        time_process(data, labels)    \n",
    "    if not time:\n",
    "        print(\"TIME = FALSE\")\n",
    "                \n",
    "    feature_process(data, time)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_process(data, labels):\n",
    "    \"\"\"\n",
    "    Time cleaning of the preprocessing\n",
    "    \"\"\"\n",
    "    # remove milliseconds\n",
    "    remove_ms(data)\n",
    "    time_features(data, labels)\n",
    "    sort_time(data)\n",
    "\n",
    "def time_features(data, labels):\n",
    "    \"\"\"\n",
    "    Decomposes and creates time objects\n",
    "    \"\"\"\n",
    "    data[labels[0]] = f_memoize_dt(data[labels[0]])\n",
    "    data[labels[1]] = data[labels[0]].dt.time\n",
    "    data[labels[2]] = data[labels[0]].dt.day_name()\n",
    "\n",
    "def sort_time(data):\n",
    "    \"\"\"\n",
    "    Sorts the data in chronological order to prevent future data\n",
    "    from leaking in\n",
    "    \"\"\"\n",
    "    data.sort_values(by=[\"event time:timestamp\"], inplace=True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "def remove_ms(df):\n",
    "    \"\"\"Removes milliseconds\"\"\"\n",
    "    df['event time:timestamp'] = df['event time:timestamp'].apply(lambda x: x.split('.')[0])\n",
    "    return df\n",
    "\n",
    "\n",
    "def f_memoize_dt(s):\n",
    "    \"\"\"\n",
    "    Memorization technique to convert to datetime\n",
    "    \"\"\"\n",
    "    dates = {date:datetime.datetime.strptime(date,\"%d-%m-%Y %H:%M:%S\") for date in s.unique()}\n",
    "    return s.map(dates)\n",
    "\n",
    "\n",
    "def dropper(df, lbls=[\"event EventID\", \"timestamp\"]):\n",
    "    df.drop(labels=lbls, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_process(data, time=True):\n",
    "    \"\"\"\n",
    "    Cleans the data and adds feautres, seperate branch for random forest\n",
    "    \"\"\"\n",
    "    log_data(data)\n",
    "    trace_data(data)\n",
    "    if time:\n",
    "        add_time_features(data)\n",
    "        cyclical_time(data)\n",
    "    else:\n",
    "        print(\"TIME = FALSE\")\n",
    "    cleaner(data)\n",
    "    \n",
    "def cleaner(data):\n",
    "    \"\"\"\n",
    "    Cleans some columns\n",
    "    \"\"\"\n",
    "    data[\"case RequestedAmount\"] = data[\"case RequestedAmount\"].astype(int)\n",
    "    data[\"next_event\"].fillna(\"LAST EVENT\", inplace=True)\n",
    "    data[\"prev_event\"].fillna(\"FIRST EVENT\", inplace=True)\n",
    "    data[\"2prev_event\"].fillna(\"FIRST EVENT\", inplace=True)\n",
    "    data[\"prev_lifecycle\"].fillna(\"FIRST EVENT\", inplace=True)\n",
    "    \n",
    "\n",
    "def log_data(data):\n",
    "    \"\"\"\n",
    "    Adds the data characteristcs based on the log\n",
    "    \"\"\"\n",
    "    # events\n",
    "    data[\"prev_event_log\"] = data[\"event concept:name\"].shift(1)\n",
    "    data[\"next_event_log\"] = data[\"event concept:name\"].shift(-1)\n",
    "    # time\n",
    "    data[\"prev_time_log\"] = data[\"event time:timestamp\"].shift(1)\n",
    "    data[\"next_time_log\"] = data[\"event time:timestamp\"].shift(-1)\n",
    "\n",
    "\n",
    "def trace_data(data):\n",
    "    \"\"\"\n",
    "    Adds the data characteristics based on the traces\n",
    "    \"\"\"\n",
    "    # events\n",
    "    data[\"prev_event\"] = data.groupby(\"case concept:name\")[\n",
    "                                \"event concept:name\"].shift(1)\n",
    "    data[\"2prev_event\"] = data.groupby(\"case concept:name\")[\n",
    "                                \"event concept:name\"].shift(2)\n",
    "    data[\"next_event\"] = data.groupby(\"case concept:name\")[\n",
    "                                \"event concept:name\"].shift(-1)\n",
    "    data[\"prev_lifecycle\"] = data.groupby('case concept:name')[\n",
    "                                'event lifecycle:transition'].shift(1)\n",
    "    # time\n",
    "    data[\"prev_time\"] = data.groupby(\"case concept:name\")[\n",
    "                                \"event time:timestamp\"].shift(1)\n",
    "    data[\"next_time\"] = data.groupby(\"case concept:name\")[\n",
    "                                \"event time:timestamp\"].shift(-1)\n",
    "\n",
    "\n",
    "def add_time_features(data):\n",
    "    \"\"\"\n",
    "    Add time features needed to train models\n",
    "    \"\"\"\n",
    "    data['day'] = data['event time:timestamp'].dt.day\n",
    "    data['month'] = data['event time:timestamp'].dt.month\n",
    "    data['hour'] = data['event time:timestamp'].dt.hour\n",
    "    data['day_of_week'] = data['event time:timestamp'].dt.weekday\n",
    "    data[\"avg_time\"] = (data['event time:timestamp'] - data['event time:timestamp'].shift()\n",
    "                    ).fillna(pd.Timedelta(seconds=0)).reset_index(drop=True)\n",
    "    data[\"avg_time\"] = data[\"avg_time\"].agg(\"mean\")\n",
    "    data[\"avg_time\"] = data[\"avg_time\"].dt.seconds\n",
    "    data[\"completion_time\"] = data[\"next_time\"] - data[\"event time:timestamp\"]\n",
    "    data[\"completion_time\"] = data[\"completion_time\"].dt.seconds\n",
    "\n",
    "def cyclical_time(data):\n",
    "    \"\"\"\n",
    "    Adds the cyclical time features\n",
    "    \"\"\"\n",
    "    data[\"hour\"] = 2 * np.pi * data[\"hour\"] / data[\"hour\"].max()\n",
    "    data[\"hour_cos\"] = np.cos(data[\"hour\"])\n",
    "    data[\"hour_sin\"] = np.sin(data[\"hour\"])\n",
    "    data[\"day_of_week\"] = 2 * np.pi * \\\n",
    "    data[\"day_of_week\"] / data[\"day_of_week\"].max()\n",
    "    data[\"day_of_week_cos\"] = np.cos(data[\"day_of_week\"])\n",
    "    data[\"day_of_week_sin\"] = np.sin(data[\"day_of_week\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_split(data, time=True):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe, then splits off the train and test sets\n",
    "    It will encode all columns such that proper predictions can be made\n",
    "    param: dataframe\n",
    "    return: encoded features in train/test split\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = split(data, time)\n",
    "    encoder(data, X_train, X_test, y_train, y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def split(data, time):\n",
    "    \"\"\"\n",
    "    splits the data for model training with a seperated random forest branch\n",
    "    \"\"\"\n",
    "    if time:\n",
    "        \n",
    "        y_labels = [\n",
    "                \"next_event\", \n",
    "                \"next_time\", \n",
    "                \"completion_time\"\n",
    "               ]\n",
    "        x_labels = data.drop(y_labels, axis=1)\n",
    "        new_data = data.dropna(subset=y_labels)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        y_labels = [\"next_event\"]\n",
    "        x_labels = data.drop(y_labels,axis=1)\n",
    "        new_data = data.dropna(subset=y_labels)\n",
    "        \n",
    "    train, test = train_test_split(new_data, \n",
    "                random_state=42, shuffle=False, test_size=0.3)\n",
    "\n",
    "    del_intersection(train, test)\n",
    "    X_train, X_test = train.drop(y_labels, axis=1), test.drop(y_labels, axis=1)\n",
    "    y_train, y_test = train[y_labels], test[y_labels]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def encoder(data, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    encode the columns that are needed in the algorithms\n",
    "    \"\"\"\n",
    "    event_encoder = LabelEncoder()\n",
    "\n",
    "    labels_name = X_train[\"prev_event\"].unique().tolist() + [\"LAST EVENT\"]\n",
    "    event_encoder.fit(labels_name)\n",
    "    X_train[[\"event concept:name\",\"prev_event\", \"2prev_event\"]] = X_train[[\"event concept:name\", \n",
    "                                    \"prev_event\", \"2prev_event\"]].apply(event_encoder.transform)\n",
    "    X_test[[\"event concept:name\", \"prev_event\", \"2prev_event\"]] = X_test[[\"event concept:name\", \n",
    "                                        \"prev_event\", \"2prev_event\"]].apply(event_encoder.transform)\n",
    "    \n",
    "    \n",
    "    labels_lifecycle = data[\"prev_lifecycle\"].unique()\n",
    "    event_encoder.fit(labels_lifecycle)\n",
    "    X_train[[\"event lifecycle:transition\", 'prev_lifecycle']] = X_train[\n",
    "        [\"event lifecycle:transition\", 'prev_lifecycle']].apply(event_encoder.transform)\n",
    "    X_test[[\"event lifecycle:transition\", 'prev_lifecycle']] = X_test[\n",
    "        [\"event lifecycle:transition\", 'prev_lifecycle']].apply(event_encoder.transform)\n",
    "    \n",
    "    labels_action = data[\"event Action\"].unique()\n",
    "    event_encoder.fit(labels_action)\n",
    "    X_train[\"event Action\"] = X_train[[\"event Action\"]].apply(event_encoder.transform)\n",
    "    X_test[\"event Action\"] = X_test[[\"event Action\"]].apply(event_encoder.transform)\n",
    "    \n",
    "    labels_resource = data[\"event org:resource\"].unique()\n",
    "    event_encoder.fit(labels_resource)\n",
    "    X_train[\"event org:resource\"] = X_train[[\"event org:resource\"]].apply(event_encoder.transform)\n",
    "    X_test[\"event org:resource\"] = X_test[[\"event org:resource\"]].apply(event_encoder.transform)\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def del_intersection(train, test):\n",
    "    lst_tr = train['case concept:name'].unique().tolist()\n",
    "    lst_te = test['case concept:name'].unique().tolist()\n",
    "\n",
    "    lst_int = set(lst_tr).intersection(lst_te)\n",
    "\n",
    "    train = train[~train['case concept:name'].isin(lst_int)]\n",
    "    test = test[~test['case concept:name'].isin(lst_int)]\n",
    "    return train, test"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47c473ffaad557a40d6f692199c8550b37e2966a5f36ac429a864ed95aaad2b0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
